{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ブースティングアルゴリズム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. 勾配ブースティング法の考え方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**abstract** : 勾配ブースティング法は、教師あり学習が仮説空間に対する損失関数の最小化問題であることに注目し、数理最適化の手法である勾配降下法を関数空間に拡張することで最適な仮説を導く方法を提案したものです。このことを理解するために、\n",
    "\n",
    "A. 数理最適化と教師あり学習の関係<br/>\n",
    "B. 勾配降下法と勾配ブースティング法\n",
    "\n",
    "を説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. 数理最適化と教師あり学習との関係"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A1. 数理最適化とは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**最適化問題の定式化** : 関数$L$は集合$S$を定義域に持ち、実数を値として返すことにしましょう。このとき、関数$L$が最小値を返すような要素$s\\in S$を**最適解**といい、$\\mathrm{argmin}_{s\\in S}L(s)$と書きます。また、このような問題を**最適化問題**、関数$L$を**目的関数**といいます。\n",
    "\n",
    "[Remark] 定義域$S$は実数を要素に持つような集合とは限りません。例えば、関数を要素に持つような集合（関数空間）も考えることができ、実際に教師あり学習の定式化に現れます。■\n",
    "\n",
    "<center><img src=\"./imgs/optim.png\" width=300px><br>最適化問題の定式化</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数理最適化の例** : 例えば、実数の全体$\\mathbb{R}$を定義域に持つ関数$L(x)=x^2+1$を考えます。この最適解は$0$ですが、これは次のように表現できます。\\begin{eqnarray*}\\mathrm{argmin}_{x\\in\\mathbb{R}}x^2+1=0\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2. 教師あり学習の復習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**線形回帰モデルの例** : 物件の月当たりの賃料をその物件の情報から予測するようなタスクを考えましょう。物件の月当たりの賃料を$y$, その物件の情報を表す値のベクトル（例えば敷地面積や最寄り駅との距離など）を$x$と書くことにします。タスクの目標は$x$から$y$を予測する関数$\\hat{h}(x)$を求めることです。\n",
    "\n",
    "例えば線形回帰モデルは、予測する関数$\\hat{h}(x)$を以下のように求めようという考え方でした : \n",
    "1. 実際に複数個($n$個)の物件について、物件の情報と月当たりの賃料$(x_1,y_1),\\cdots,(x_n,y_n)$を集めてきます。\n",
    "2. 予測する関数$\\hat{h}(x)$を1次式のなかから探すと決めるのでした。要するに、\\begin{eqnarray*}\\hat{h}(x)=w_0+w_1x_1+\\cdots+w_dx_d\\end{eqnarray*}です。\n",
    "3. 賃料の予測値$\\hat{y_i}$と実際の賃料の値$y_i$の「食い違い」を計る関数$l(\\hat{y_i},y_i)$を\\begin{eqnarray*}l(\\hat{y_i},y_i)=(\\hat{y_i}-y_i)^2\\end{eqnarray*}に決めます。これを2乗損失というのでした。\n",
    "4. 1次式全体を$\\mathcal{H}$と書くことにします。$\\mathcal{H}$のなかから、`step 1`で収集した物件全体に対する「食い違い」\\begin{eqnarray*}L(h)&=&\\sum_{i=1}^{n}l(h(x_i),y_i)\\\\&=&\\sum_{i=1}^n(h(x_i)-y_i)^2\\end{eqnarray*}が最も小さくなるような関数$\\hat{h}(x)$を求めます。これを賃料を予測する関数として用いようというわけです。\n",
    "\n",
    "<center><img src=\"./imgs/supervised.png\" width=400px></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**教師あり学習の定式化** : 機械学習では、予測したい変数を**出力** (output) といいます。また、予測に用いる関数を**仮説** (hypothesis) 、仮説に渡すベクトルを**入力** (input) と言います。入力と出力のペア$(x_i,y_i)$を複数観測したものを**データ**といい、$\\mathcal{D}=\\{(x_1,y_1),\\cdots,(x_n,y_n)\\}$と表現します。\n",
    "\n",
    "事前にデータ$\\mathcal{D}=\\{(x_1,y_1),\\cdots,(x_n,y_n)\\}$を準備し、\n",
    "1. 想定される仮説の全体$\\mathcal{H}$\n",
    "2. 予測値$\\hat{y}$と出力の観測値$y$の「食い違い」$l(\\hat{y_i},y)$\n",
    "\n",
    "を定義したうえで、以下の最適化問題を解くことを**教師あり学習**といいます。\\begin{eqnarray*}\\hat{h}(x)=\\mathrm{argmin}_{h\\in\\mathcal{H}}\\sum_{i=1}^{n}l(h(x_i),y_i)\\end{eqnarray*}このとき、使われたデータ$\\mathcal{D}$を**訓練データ**、想定される仮説の全体$\\mathcal{H}$を**仮説空間**、「食い違い」を計るために定義した関数$l(\\hat{y_i},y_i)$を**損失関数**といいます。求めた仮説$\\hat{h}(x)$を出力を予測するための関数として採用します。\n",
    "\n",
    "[Remark] このように教師あり学習は仮説空間と呼ばれる予測式の候補として想定した関数の集合に対して、データと損失関数に基づいて定義した目的関数$L(h)$の最適化問題を解いていると考えることが出来ます。この数理最適化による定式化は、後で勾配ブースティング法を理解するヒントになります。■"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. 勾配降下法と勾配ブースティング法 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B1. 勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**勾配降下法とそのalgorithm** : $d$個の実数からなるベクトルの全体$\\mathbb{R}^d$を定義域に持つような関数$L(x)$の最適解の値を計算したいとします。特に、関数$L(x)$が1階微分可能、要するに導関数$\\nabla_x L(x)$を持ち、この導関数に$x=a$を代入した値$\\nabla_x L(a)$を計算できるとしましょう。このとき、以下のようなalgorithmが用いられることがあります。\n",
    "\n",
    "**Input** :\n",
    "* $x_0$ : 初期値\n",
    "* $\\eta>0$ : 学習率\n",
    "* $\\nabla_x L$ : 目的関数$L$の導関数\n",
    "* $M$ : 繰り返しの最大回数\n",
    "\n",
    "**Process** :\n",
    "1. $a\\leftarrow x_0$\n",
    "2. for $t$ in $1,\\cdots,M$:\n",
    "3. 　　$a\\leftarrow a-\\eta\\nabla_x L(a)$\n",
    "4. return $a$\n",
    "\n",
    "このようなalgorithmを**勾配降下法** (gradient descent) といいます。以下にPython言語における実装例を紹介します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpyを用いた実装\n",
    "import numpy as np\n",
    "def gradient_descent(x0, eta, deriv, M):\n",
    "    a = x0\n",
    "    for t in range(M):\n",
    "        a = a - eta * deriv(a)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**勾配降下法のイメージ** : 勾配降下法は以下の図のように、$k$回繰り返した時点での解$x^{(k)}$において、より目的関数$f(x)$の値が現状より小さくなる方向$-\\nabla_{x}f(a)$へ解を更新するalgorithmです。\n",
    "<center><img src=\"./imgs/gradient_descent.png\" width=400px><br>勾配降下法の計算の仕組み, $\\mathrm{grad}(f)=\\nabla_{x}fとする。$<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**勾配降下法の性質** : 勾配降下法の性質を述べるうえで重要なキーワードは局所最適解です。適当な開集合$S'$に目的関数を制限したとき、その最小値を与えるような点$s'$が$S'$の要素として存在すれば、それを**局所最適解**といいます。例えば、以下のようなグラフを持つ関数\n",
    "\\begin{eqnarray*}\n",
    "f(x) &=& \\frac{1}{4}x^4 - \\frac{1}{3}x^3 - x^2\n",
    "\\end{eqnarray*}\n",
    "では、最適解は$x=2$なのに対して、局所最適解は$x=-1, 2$の2つです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的関数\n",
    "def example(x):\n",
    "    return (1/4)*x**4 - (1/3)*x**3 - x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 目的関数のグラフ\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(start = -2, stop = 3, num = 100)\n",
    "plt.plot(x, example(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようなグラフを持つ関数に対して勾配降下法を用いた場合、計算結果は必ずしも最適解$x=2$に収束するとは限らず、局所最適解$x=-1,2$のいずれかに収束します。以下の計算例を確認してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的関数の導関数\n",
    "def nabla_example(x):\n",
    "    return x**3 - x**2 - 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0000000000000002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 勾配降下法 : 初期値がx=-2の場合, 学習率は0.1\n",
    "gradient_descent(x0 = -2, eta = 0.1, deriv = nabla_example, M = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 勾配降下法 : 初期値がx=3の場合, 学習率は0.1\n",
    "gradient_descent(x0 = 3, eta = 0.1, deriv = nabla_example, M = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B2. 勾配ブースティング法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**勾配ブースティング法のアイディア** : 勾配ブースティング法は、`B1`節で紹介した勾配降下法のようなalgorithmを教師あり学習における最適化問題に実現したものです。教師あり学習における最適化問題とは、事前にデータ$\\mathcal{D}=\\{(x_1,y_1),\\cdots,(x_n,y_n)\\}$を準備し、\n",
    "1. 仮説空間$\\mathcal{H}$\n",
    "2. 損失関数$l(\\hat{y_i},y)$\n",
    "\n",
    "を定義したうえで、以下のような問題を解くことを言うのでした。\\begin{eqnarray*}\\hat{h}(x)&=&\\mathrm{argmin}_{h\\in\\mathcal{H}}L(h)\\\\\n",
    "\\text{ where }L(h)&=&\\sum_{i=1}^{n}l(h(x_i),y_i)\\end{eqnarray*}勾配降下法をこの問題に拡張するということは、勾配降下法が対象としている目的関数の定義域を実数ベクトルの全体$\\mathbb{R}^n$から仮説空間（要するに予測式の候補になる関数の空間）$\\mathcal{H}$に拡張するためにはどうするかを考えるということです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**仮説空間を定義域にもつ目的関数の導関数** : 勾配降下法は目的関数の導関数を用いるalgorithmでした。そこで、勾配降下法の対象を実数ベクトルを代入できる目的関数から、仮説（関数）$h$を代入できる目的関数$L(h)$に拡張するには、**仮説（関数）$h$を代入できる関数の導関数$\\nabla_{h}L$を定義する必要がある**わけです。\n",
    "\n",
    "実数ベクトルを代入できる関数の導関数$\\nabla_{x}L$は$x_1,\\cdots,x_d$軸方向に対する関数$L(x)$の変化率を意味するものでした。\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\nabla_{x}L(x) &:=& \\left(\\frac{\\partial L}{\\partial x_1}(x),\\cdots,\\frac{\\partial L}{\\partial x_d}(x)\\right)^T\n",
    "\\end{eqnarray*}\n",
    "\n",
    "この考え方は、実数ベクトル$x$が$x_i$軸に対応する単位ベクトル$e_i$を用いて$x=\\sum_{i=1}^{d}x_ie_i$と書けることからきています。\n",
    "\n",
    "同様のことを仮説を代入できる関数に対して考えてみましょう。ここで役に立つのは仮説$h(x)$が指示関数\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mathbb{1}[x=a]&=&\\begin{cases}1\\text{ if }x=a\\\\0\\text{ otherwise}\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "を用いて$h(x)=\\sum_{a\\in\\mathbb{R}^d}h(a)\\mathbb{1}[x=a]$と書けるという事実です。要するに、仮説を代入できる関数の導関数$\\nabla_{h}L$は$\\mathbb{1}[x=a]$, $a\\in\\mathbb{R}^d$を軸とする方向に仮説$L(h)$がどれくらいの変化率を持つかを意味するように定義すればよいでしょう。これは数式で次のように書くことができます。\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\nabla_{h}L(h)(a) &:=& \\left.\\frac{\\partial L(h+\\alpha\\mathbb{1}[x=a])}{\\partial \\alpha}\\right|_{\\alpha=0}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "重要な例を1つやっておきましょう。\n",
    "\n",
    "**2乗損失の場合の例** : サイズ$n$のデータ$\\mathcal{D}=\\{(x_1,y_1),\\cdots,(x_n,y_n)\\}$と2乗損失$l(\\hat{y},y)=(y-\\hat{y_i})^2$から定義される教師あり学習の最適化問題の目的関数\n",
    "\\begin{eqnarray*}\n",
    "L(h) &=& \\sum_{i=1}^{n}(y_i-h(x_i))^2\n",
    "\\end{eqnarray*}\n",
    "\n",
    "に対して、導関数は次のように計算することが出来ます。\n",
    "\\begin{eqnarray*}\n",
    "\\nabla_{h}L(h)(a) &=& \\left.\\frac{\\partial \\sum_{i=1}^{n}\\{y_i-(h(x_i)+\\alpha\\mathbb{1}[x_i=a])\\}^2}{\\partial \\alpha}\\right|_{\\alpha=0}\\\\\n",
    "&=& -2\\{y_i-(h(x_i)+\\alpha\\mathbb{1}[x_i=a])\\}\\mathbb{1}[x_i=a]|_{\\alpha=0}\\\\\n",
    "&=& -2(y_i-h(x_i))\\mathbb{1}[x_i=a]\\\\\n",
    "&=& \\begin{cases}-2(y_i-h(x_i))\\text{ if }a=x_i\\\\0\\text{ otherwise }\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "L(h)の勾配＝１つの関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**勾配ブースティング法** : さて、これまでの準備を用いて、勾配ブースティング法を紹介しましょう。次のような最適化問題を考えます。\n",
    "\n",
    "<div style=\"padding: 10px; margin-bottom: 10px; border: 1px dotted #333333;\">\n",
    "事前にデータ$\\mathcal{D}=\\{(x_1,y_1),\\cdots,(x_n,y_n)\\}$を準備し、仮説を表現するために基底となる関数$h_i$の形を事前に決めておきます。\n",
    "    \n",
    "1. 仮説空間$\\mathcal{H}=\\{\\sum_{i}w_ih_i(x)\\mid h_{i}:基底となる関数\\}$\n",
    "2. 損失関数$l(\\hat{y_i},y)$\n",
    "\n",
    "を定義したうえで、以下のような問題を解く。\n",
    "    $$\\hat{h}(x)=\\mathrm{argmin}_{h\\in\\mathcal{H}}L(h)$$\n",
    "    $$\\text{where }L(h)=\\sum_{i=1}^{n}l(h(x_i),y_i)$$\n",
    "</div>\n",
    "\n",
    "例えば、基底となる関数$h_i$には**決定木**を用います。この問題を次のようなalgorithmで解くのが勾配ブースティング法です。\n",
    "\n",
    "**Input** :\n",
    "* $\\eta>0$ : 学習率\n",
    "* $\\nabla_h L$ : 目的関数$L$の導関数\n",
    "* $M$ : 繰り返しの最大回数\n",
    "\n",
    "**Process** :\n",
    "1. $\\hat{h}(x)\\leftarrow \\mathrm{argmax}_{c:constant}L(c)$　　# 初期値は目的関数を最小化する定数関数\n",
    "2. for $t$ in $1,\\cdots,M$:\n",
    "3. 　　$r=(\\nabla_{h}L(x_1),\\cdots,\\nabla_{h}L(x_n))$を計算する。\n",
    "4. 　　$\\mathcal{D}':=\\{(x_1,-r_1),\\cdots,(x_n,-r_n)\\}$で$h_{t}(x)$を学習する。\n",
    "5. 　　$w_{t}:=\\mathrm{argmin}_{w\\in\\mathbb{R}}L(\\hat{h}+wh_{t})$を求める。\n",
    "6. 　　$\\hat{h}(x)\\leftarrow\\hat{h}(x)+\\eta w_{t}h_{t}(x)$\n",
    "7. return $\\hat{h}(x)$\n",
    "\n",
    "ここで、勾配降下法が$a\\leftarrow a-\\eta\\nabla_x L(a)$としているところを、勾配ブースティング法では直接行わなわず、代わりに`step 4`と`step 5`を行っていることが見て取れます。これは関数$h\\in\\mathcal{H}$に対して、\n",
    "\\begin{eqnarray*}\n",
    "h(x)-\\eta\\nabla_{h}L(x)\n",
    "\\end{eqnarray*}\n",
    "が仮説空間$\\mathcal{H}$に属する関数になるとは限らないためです。更新したい$-\\nabla_{h}L(x)$と目的関数の意味で大きな差がない基底関数の定数倍$w_{t}h_{t}$を求めることで、更新後の仮説$\\hat{h}(x)$が仮説空間に属するようにしています。\n",
    "\n",
    "ステップ４＝決定木でLの勾配を近似している（学習している）→$\\mathcal{H}$の中に収まる！\n",
    "\n",
    "ステップ５＝目的関数が最小になる定数倍$wh_{t}$を求める"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2乗損失の場合の勾配ブースティング法** : 勾配ブースティング法を2乗損失の場合で具体的に書き下しましょう。要するにサイズ$n$のデータ$\\mathcal{D}=\\{(x_1,y_1),\\cdots,(x_n,y_n)\\}$と2乗損失$l(\\hat{y},y)=(y-\\hat{y})^2$から定義される教師あり学習の最適化問題の目的関数\n",
    "\\begin{eqnarray*}\n",
    "L(h) &=& \\sum_{i=1}^{n}(y_i-h(x_i))^2\n",
    "\\end{eqnarray*}\n",
    "\n",
    "に対して、勾配ブースティング法による学習を考えます。ここで、次の2つの事実を用います。\n",
    "1. $\\mathrm{argmax}_{c:constant}L(c)=\\bar{y}$\n",
    "2. $\\nabla_{h}L(h)(a)=\\begin{cases}-2(y_i-h(x_i))\\text{ if }a=x_i\\\\0\\text{ otherwise }\\end{cases}$\n",
    "\n",
    "特に`事実2`は上の**2乗損失の場合の例**で導出したものです。これから、2乗損失の場合の勾配ブースティング法のalgorithmは次のように書き下すことができます。\n",
    "\n",
    "**Input** :\n",
    "* $\\eta>0$ : 学習率\n",
    "* $\\nabla_h L$ : 目的関数$L$の導関数\n",
    "* $M$ : 繰り返しの最大回数\n",
    "\n",
    "**Process** :\n",
    "1. $\\hat{h}(x)\\leftarrow \\bar{y}$\n",
    "2. for $t$ in $1,\\cdots,M$:\n",
    "3. 　　$\\mathcal{D}':=\\{(x_1,2(y_i-\\hat{h}(x_i))),\\cdots,(x_n,2(y_n-\\hat{h}(x_n)))\\}$で$h_{t}(x)$を学習する。\n",
    "4. 　　$w_{t}:=\\mathrm{argmin}_{w\\in\\mathbb{R}}L(\\hat{h}+wh_{t})$を求める。\n",
    "5. 　　$\\hat{h}(x)\\leftarrow\\hat{h}(x)+\\eta w_{t}h_{t}(x)$\n",
    "6. return $\\hat{h}(x)$\n",
    "\n",
    "`note2_gbm_demo.ipynb`では、このalgorithmの`sklearn` packageを用いたデモを行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[Fri2001] J. Friedman, \"Greedy Function Approximation: A Gradient Boosting Machine.\" The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
    "\n",
    "[Mas2000] Mason, Llew, et al. \"Boosting algorithms as gradient descent.\" Advances in neural information processing systems. 2000."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
